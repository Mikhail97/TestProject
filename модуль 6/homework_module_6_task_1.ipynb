{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mikhail/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics for MeanEmbeddingVectorizer:\n",
      "Precision:   0.54\n",
      "Recall:   0.52\n",
      "F1-measure:   0.52\n",
      "Accuracy:   0.53\n",
      "\n",
      "Val metrics for MeanEmbeddingVectorizer:\n",
      "Precision:   0.56\n",
      "Recall:   0.55\n",
      "F1-measure:   0.55\n",
      "Accuracy:   0.59\n",
      "\n",
      "Inference metrics for MeanEmbeddingVectorizer:\n",
      "Precision:   0.52\n",
      "Recall:   0.53\n",
      "F1-measure:   0.52\n",
      "Accuracy:   0.53\n",
      "\n",
      "Test metrics for TfidfEmbeddingVectorizer:\n",
      "Precision:   0.54\n",
      "Recall:   0.53\n",
      "F1-measure:   0.53\n",
      "Accuracy:   0.54\n",
      "\n",
      "Val metrics for TfidfEmbeddingVectorizer:\n",
      "Precision:   0.55\n",
      "Recall:   0.52\n",
      "F1-measure:   0.53\n",
      "Accuracy:   0.57\n",
      "\n",
      "Inference metrics for TfidfEmbeddingVectorizer:\n",
      "Precision:   0.56\n",
      "Recall:   0.56\n",
      "F1-measure:   0.55\n",
      "Accuracy:   0.56\n",
      "\n",
      "Test metrics for Doc2VecVectorizer:\n",
      "Precision:   0.81\n",
      "Recall:   0.79\n",
      "F1-measure:   0.79\n",
      "Accuracy:   0.80\n",
      "\n",
      "Val metrics for Doc2VecVectorizer:\n",
      "Precision:   0.80\n",
      "Recall:   0.80\n",
      "F1-measure:   0.78\n",
      "Accuracy:   0.79\n",
      "\n",
      "Inference metrics for Doc2VecVectorizer:\n",
      "Precision:   0.83\n",
      "Recall:   0.82\n",
      "F1-measure:   0.82\n",
      "Accuracy:   0.82\n"
     ]
    }
   ],
   "source": [
    "from base64 import decode\n",
    "import glob\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import pymorphy2\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter, defaultdict\n",
    "from gensim.models.doc2vec import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "regex = re.compile(\"[А-Яа-я:=!\\)\\()A-z\\_\\%/|]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    return \" \".join(regex.findall(str(text)))\n",
    "\n",
    "\n",
    "def lemmatize(doc):\n",
    "    #doc = re.sub(patterns, ' ', doc)\n",
    "    #print('doc = ',doc)\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token and token not in stopwords_en:\n",
    "            token = token.strip()\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            tokens.append(token)\n",
    "    if len(tokens) > 2:\n",
    "        return ' '.join(tokens).strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(word2vec.popitem()[1])\n",
    "        # self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(w2v.popitem()[1])\n",
    "        # self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n",
    "\n",
    "class Doc2VecVectorizer(object):\n",
    "    def __init__(self, d2v_model):\n",
    "        self.d2v_model = d2v_model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([self.d2v_model.infer_vector(text.split()) for text in X])\n",
    "\n",
    "\n",
    "path = '/home/mikhail/it-academy/модуль 6/bbc/'\n",
    "dfs = []\n",
    "for i, dirname in enumerate(listdir(path) ):\n",
    "    filenames = glob.glob(path + dirname + '/*.txt')\n",
    "    for filename in filenames:\n",
    "         text = open(filename, mode=\"rb\").read()\n",
    "         dfs.append((text, dirname))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(dfs, columns=['text', 'target'])\n",
    "\n",
    "#regex = re.compile(\"[А-Яа-я:=!\\)\\()A-z\\_\\%/|]+\")\n",
    "stopwords_en = stopwords.words('english')\n",
    "morph = MorphAnalyzer()\n",
    "df.text = df.text.apply(words_only)\n",
    "df.text = df.text.apply(lemmatize)\n",
    "df.text = df.text.apply(lambda x: x.replace('\\\\n',' '))\n",
    "df.text = df.text.apply(lambda x: x[1:])\n",
    "df.text = df.text.apply(lambda x: x.replace('\\\\', ''))\n",
    "#df.to_csv('/home/mikhail/it-academy/модуль 6/bbc/news.csv')\n",
    "\n",
    "X = df.text.tolist()\n",
    "y = df.target.tolist()\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test,y_test, test_size=0.5)\n",
    "X_val, X_inference, y_val, y_inference = train_test_split(X_val,y_val, test_size=0.5)\n",
    "\n",
    "#*********************** MeanEmbeddingVectorizer and TfidfEmbeddingVectorizer **************************************************\n",
    "texts = [str(df.text.iloc[i].split()) for i in range(len(df))]\n",
    "word_to_vec = Word2Vec(texts, vector_size=100, window=5, min_count=1, workers=-1)\n",
    "word_to_vec.save('./word2vec.pk')\n",
    "w2v = dict(zip(word_to_vec.wv.index_to_key, word_to_vec.wv.vectors))\n",
    "#*******************************************************************************************************************************\n",
    "\n",
    "#***************** Doc2VecVectorizer *******************************************************************************************\n",
    "splitted_texts = [text.split() for text in X]\n",
    "idx = [str(i) for i in range(len(X))]\n",
    "docs = []\n",
    "# Чтобы модель поняла отношения документов по Id нам надо пронумеровать эти документы, это делается следующим образом:\n",
    "for i in range(len(X)):\n",
    "    docs.append(TaggedDocument(splitted_texts[i], [idx[i]]))  \n",
    "model = Doc2Vec(vector_size=300, window=5, min_count=5, workers=8, alpha=0.025, min_alpha=0.01, dm=0)\n",
    "# плюс в начале теперь нам надо построить словарь id докумеентов\n",
    "model.build_vocab(docs)\n",
    "model.train(docs, total_examples=len(docs), epochs=20)\n",
    "#*******************************************************************************************************************************\n",
    "\n",
    "\n",
    "rfc_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "                    (\"extra trees\", RandomForestClassifier(n_estimators=20))])\n",
    "\n",
    "rfc_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "                          (\"extra trees\", RandomForestClassifier(n_estimators=20))])\n",
    "\n",
    "rfc_d2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", Doc2VecVectorizer(model)),\n",
    "    (\"extra trees\", RandomForestClassifier(n_estimators=20))])\n",
    "\n",
    "\n",
    "\n",
    "rfc_w2v.fit(X_train,y_train)\n",
    "\n",
    "pred = rfc_w2v.predict(X_test)\n",
    "pred_val = rfc_w2v.predict(X_val)\n",
    "pred_inference = rfc_w2v.predict(X_inference)\n",
    "\n",
    "print('Test metrics for MeanEmbeddingVectorizer:')\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "\n",
    "print()\n",
    "print('Val metrics for MeanEmbeddingVectorizer:')\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_val, pred_val, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_val, pred_val, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_val, pred_val, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_val, pred_val)))\n",
    "\n",
    "print()\n",
    "print('Inference metrics for MeanEmbeddingVectorizer:')\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_inference, pred_inference, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_inference, pred_inference, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_inference, pred_inference, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_inference, pred_inference)))\n",
    "\n",
    "\n",
    "#print(classification_report(y_test, pred))\n",
    "\n",
    "#abels = rfc_w2v.classes_\n",
    "\n",
    "#sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "#plt.title(\"Confusion matrix\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "rfc_w2v_tfidf.fit(X_train,y_train)\n",
    "\n",
    "pred = rfc_w2v_tfidf.predict(X_test)\n",
    "pred_val = rfc_w2v_tfidf.predict(X_val)\n",
    "pred_inference = rfc_w2v_tfidf.predict(X_inference)\n",
    "\n",
    "print()\n",
    "print('Test metrics for TfidfEmbeddingVectorizer:')\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "\n",
    "print()\n",
    "print('Val metrics for TfidfEmbeddingVectorizer:')\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_val, pred_val, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_val, pred_val, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_val, pred_val, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_val, pred_val)))\n",
    "\n",
    "print()\n",
    "print('Inference metrics for TfidfEmbeddingVectorizer:')\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_inference, pred_inference, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_inference, pred_inference, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_inference, pred_inference, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_inference, pred_inference)))\n",
    "\n",
    "#print(classification_report(y_test, pred))\n",
    "\n",
    "#labels = rfc_w2v.classes_\n",
    "\n",
    "#sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "#plt.title(\"Confusion matrix\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "rfc_d2v.fit(X_train,y_train)\n",
    "\n",
    "pred = rfc_d2v.predict(X_test)\n",
    "pred_val = rfc_d2v.predict(X_val)\n",
    "pred_inference = rfc_d2v.predict(X_inference)\n",
    "\n",
    "print()\n",
    "print('Test metrics for Doc2VecVectorizer:')\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "\n",
    "print()\n",
    "print('Val metrics for Doc2VecVectorizer:')\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_val, pred_val, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_val, pred_val, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_val, pred_val, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_val, pred_val)))\n",
    "\n",
    "print()\n",
    "print('Inference metrics for Doc2VecVectorizer:')\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_inference, pred_inference, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_inference, pred_inference, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_inference, pred_inference, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_inference, pred_inference)))\n",
    "\n",
    "#print(classification_report(y_test, pred))\n",
    "#labels = rfc_w2v.classes_\n",
    "\n",
    "\n",
    "\n",
    "#sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "#plt.title(\"Confusion matrix\")\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть 2  !!! На гитхабе есть версия ноутбука для гугл колаб, так как на моем компе не хватает ресурсов чтобы отладить этот код !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = pd.read_csv('/home/mikhail/it-academy/модуль 6/bbc/news.csv')\n",
    "data=data.sample(frac=1.0, random_state=42)\n",
    "#data = data.iloc[:500]\n",
    "\n",
    "\n",
    "labels=data.target.unique()\n",
    "NUM_LABELS= len(labels)\n",
    "id2label={i:l for i,l in enumerate(labels)}\n",
    "label2id={l:i for i,l in enumerate(labels)}\n",
    "data[\"labels\"]=data.target.map(lambda x: label2id[x])\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\", max_length=512)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",  num_labels=NUM_LABELS, id2label=id2label, label2id=label2id)\n",
    "#model.to('cpu')\n",
    "\n",
    "SIZE= data.shape[0]\n",
    "\n",
    "df_train, df_test, y_train, y_test = train_test_split(data.text, data.labels, test_size=0.3)\n",
    "df_test, df_val, y_test, y_val = train_test_split(df_test, y_test, test_size=0.5)\n",
    "df_val, df_inference, y_val, y_inference = train_test_split(df_val, y_val, test_size=0.5)\n",
    "\n",
    "train_texts= list(df_train)\n",
    "val_texts=   list(df_val)\n",
    "test_texts=  list(df_test)\n",
    "inference_texts=  list(df_inference)\n",
    "\n",
    "train_labels= list(y_train)\n",
    "val_labels=   list(y_val)\n",
    "test_labels=  list(y_test)\n",
    "inference_labels=  list(y_inference)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_encodings  = tokenizer(val_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "inference_encodings = tokenizer(inference_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }\n",
    "\n",
    "train_dataset = MyDataset(train_encodings, train_labels)\n",
    "val_dataset = MyDataset(val_encodings, val_labels)\n",
    "test_dataset = MyDataset(test_encodings, test_labels)\n",
    "inference_dataset = MyDataset(inference_encodings, inference_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # The output directory where the model predictions and checkpoints will be written\n",
    "    output_dir='/content/outputs/',\n",
    "    #  The number of epochs, defaults to 3.0\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    # Number of steps used for a linear warmup\n",
    "    #warmup_steps=10,\n",
    "    weight_decay=0.01\n",
    "    #no_cuda=True\n",
    "   # TensorBoard log directory\n",
    "    #fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    # the pre-trained model that will be fine-tuned\n",
    "    model=model,\n",
    "     # training arguments that we defined above\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics= compute_metrics\n",
    ")\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "q=[trainer.evaluate(eval_dataset=data) for data in [train_dataset, val_dataset, test_dataset, inference_dataset]]\n",
    "print(pd.DataFrame(q, index=[\"train\",\"val\",\"test\",\"inference\"]).iloc[:,:5])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    return probs\n",
    "\n",
    "\n",
    "inference_texts = [\n",
    "('Inflation in Turkey has climbed above 83% - a 24-year-high. The transport, food and housing sectors have seen the biggest rise in prices. Independent experts the Inflation Research Group estimate the annual rate is actually 186.27%.',label2id['business']), #business\n",
    "('The pound has climbed after the chancellor reversed his controversial decision to scrap the top rate of tax.Sterling gained more than 1% to $1.1284 before falling back slightly while government borrowing costs edged lower.Tory MPs had threatened to vote against Kwasi Kwarteng\\'s plan, saying it was unfair when living costs were so high.',label2id['business']),    #business\n",
    "('Australia coach Mal Meninga has named 13 uncapped players in his squad as they chase a third men\\'s Rugby League World Cup title in a row.The Kangaroos beat England in the 2017 final but have only played four Tests since with their last match a shock loss to Tonga three years ago.Sydney Roosters full-back James Tedesco, who represented Italy at the last two World Cups, captains the side.',label2id['sport']),  #sport\n",
    "('England will play India in the group stage of the 2023 Women\\'s T20 World Cup in South Africa.The two sides have been placed into Group B alongside West Indies, Pakistan and Ireland.Defending champions Australia, New Zealand, hosts South Africa, Sri Lanka and Bangladesh make up Group A with the top two in each group progressing to the semi-finals.The tournament takes place between 10 and 26 February.',label2id['sport']), #sport\n",
    "('During World War II, Spitfire pilots described their plane as so responsive it felt like an extension of their limbs.Fighter pilots of the 2030s, however, will have an even closer relationship with their fighter jet.It will read their minds.',label2id['tech']), #tech\n",
    "('In deep, astonishingly clear, blue-lit ponds some 40m (130ft) beneath the Swedish countryside, lies decades worth of high-level nuclear waste.It is an oddly beautiful and rather disturbing sight. Row upon row of long metal containers, filled with used nuclear fuel from the country\\'s reactors, lie below the surface near Oskarshamn, on Sweden\\'s Baltic coast.It is both highly lethal and entirely safe.',label2id['tech']) #tech\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(inference_texts, columns=['texts', 'labels'])\n",
    "\n",
    "inference_acc = 0\n",
    "n_iter = 0\n",
    "for text, label in zip(df.texts, df.labels):\n",
    "    probs = predict(text)\n",
    "    #print(probs.argmax(), label, n_iter)\n",
    "    inference_acc += (probs.argmax() == label).sum().item()\n",
    "    n_iter += 1\n",
    "\n",
    "print('Inference_acc: {}'.format(inference_acc / n_iter ))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть 3. Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подход DistillBert работает намного лучше чем классическое машинное обучение с моделью на RandomForestClassifier. \n",
    "Метрика Acc 0.82 для классического машинного обучения c векторным представлением слов doc2vec.\n",
    "Метрика Acc 0.95 для DistillBert.\n",
    "Предположу, что это связано с тем, что DistillBert обучался на гораздо большем объеме данных чем у меня есть для данной задачи. И еще вдобавок  DistillBert дообучал на данных, что есть у меня. Но классическая модель обучилась гораздо быстрее чем тяжелый DistillBert (4 часа в гугл колабе), что привело к потере времени при отладке. Так же мне не хватило ресурсов компьютера чтобы дообучить DistillBert и пришлось использовать гугл колаб (пока бесплатно, но для больших датасетов нужно было бы платить за подписку, а это дополнительные расходы для компании). Поэтому если датасет не большой и для задачи нет необходимости высокой точности, то достаточно классического варианта. В противном случаи DistillBert."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
