{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model is RNN\n",
      "Epoch 0    Train Loss: 1.838  Train acc: 0.150  Test acc: 0.267\n",
      "Epoch 10    Train Loss: 0.062  Train acc: 0.964  Test acc: 0.933\n",
      "Epoch 20    Train Loss: 0.037  Train acc: 0.957  Test acc: 0.967\n",
      "Epoch 30    Train Loss: 0.033  Train acc: 0.979  Test acc: 0.933\n",
      "Epoch 40    Train Loss: 0.031  Train acc: 0.971  Test acc: 0.967\n",
      "Epoch 50    Train Loss: 0.030  Train acc: 0.986  Test acc: 1.000\n",
      "Epoch 60    Train Loss: 0.029  Train acc: 0.986  Test acc: 0.967\n",
      "Epoch 70    Train Loss: 0.029  Train acc: 0.986  Test acc: 1.000\n",
      "Epoch 80    Train Loss: 0.028  Train acc: 0.979  Test acc: 0.933\n",
      "Epoch 90    Train Loss: 0.028  Train acc: 0.993  Test acc: 0.967\n",
      "Valid acc: 1.000\n",
      "\n",
      "model is LSTM\n",
      "Epoch 0    Train Loss: 2.180  Train acc: 0.107  Test acc: 0.133\n",
      "Epoch 10    Train Loss: 0.102  Train acc: 0.929  Test acc: 0.967\n",
      "Epoch 20    Train Loss: 0.054  Train acc: 0.986  Test acc: 0.933\n",
      "Epoch 30    Train Loss: 0.036  Train acc: 0.986  Test acc: 0.967\n",
      "Epoch 40    Train Loss: 0.032  Train acc: 0.971  Test acc: 1.000\n",
      "Epoch 50    Train Loss: 0.031  Train acc: 0.964  Test acc: 1.000\n",
      "Epoch 60    Train Loss: 0.030  Train acc: 0.993  Test acc: 0.967\n",
      "Epoch 70    Train Loss: 0.029  Train acc: 0.993  Test acc: 1.000\n",
      "Epoch 80    Train Loss: 0.029  Train acc: 0.993  Test acc: 1.000\n",
      "Epoch 90    Train Loss: 0.028  Train acc: 0.986  Test acc: 1.000\n",
      "Valid acc: 1.000\n",
      "\n",
      "model is GRU\n",
      "Epoch 0    Train Loss: 2.129  Train acc: 0.071  Test acc: 0.167\n",
      "Epoch 10    Train Loss: 0.063  Train acc: 0.950  Test acc: 0.900\n",
      "Epoch 20    Train Loss: 0.037  Train acc: 0.986  Test acc: 0.967\n",
      "Epoch 30    Train Loss: 0.040  Train acc: 0.979  Test acc: 1.000\n",
      "Epoch 40    Train Loss: 0.031  Train acc: 0.979  Test acc: 1.000\n",
      "Epoch 50    Train Loss: 0.029  Train acc: 0.964  Test acc: 0.967\n",
      "Epoch 60    Train Loss: 0.029  Train acc: 0.986  Test acc: 0.967\n",
      "Epoch 70    Train Loss: 0.029  Train acc: 0.993  Test acc: 1.000\n",
      "Epoch 80    Train Loss: 0.029  Train acc: 0.993  Test acc: 1.000\n",
      "Epoch 90    Train Loss: 0.028  Train acc: 0.993  Test acc: 1.000\n",
      "Valid acc: 0.967\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split         # разбить данные на тестовые и тренеровочные\n",
    "from random import randint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def generate_sequence(len_sequence):\n",
    "    X = torch.randint(low=0, high=9, size=(len_sequence, 1), dtype=int)\n",
    "    Y = torch.zeros((len_sequence), dtype=int)\n",
    "    #print('X = ',X)\n",
    "    Y[0] = X[0]\n",
    "    for i in range(1, len(X)):\n",
    "        val = X[i].item() + X[0].item() \n",
    "        if val>=10:\n",
    "            val -= 10    \n",
    "        Y[i] = val\n",
    "    return X, Y\n",
    "\n",
    "def sample(preds):\n",
    "    softmaxed = torch.softmax(preds, 1) # распределяем значения от 0 до 1\n",
    "    #print('softmaxed  ',softmaxed)\n",
    "    probas = torch.distributions.multinomial.Multinomial(1, softmaxed).sample()  # нормализация\n",
    "    #print('probas   ',probas)\n",
    "    #print(probas.max(dim=1)[1])\n",
    "    return probas.max(dim=1)[1]#.argmax()\n",
    "\n",
    "# Строим класс RNN который будет принимать различную указанную вариацию рекуррентной ячейки - GRU / LSTM/ SimpleRNN\n",
    "class RnnFlex(torch.nn.Module):\n",
    "                        # тип     размер словаря  размер эмб       скрытые слои   классы\n",
    "    def __init__(self, rnnClass, dictionary_size, embedding_size, num_hiddens, num_classes, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = torch.nn.Embedding(dictionary_size, embedding_size) # учится представлять наши выходные параметры в виде векторов\n",
    "        self.hidden = rnnClass(embedding_size, num_hiddens, batch_first=True) # batch_first=True когды мы начинаем с нулевого хидден стейта\n",
    "        self.output = torch.nn.Linear(num_hiddens, num_classes)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.embedding(X)  # прошкалировали закодированный Х и добавили размерность 1,40, 64\n",
    "        #print(out.shape)\n",
    "        _, state = self.hidden(out)  # приходят все выходы и последний выход (между LSTM  и GRU выход немного разный)\n",
    "        #print(state[0])\n",
    "        predictions = self.output(state[0])\n",
    "        return predictions\n",
    "\n",
    "\n",
    "X,Y = generate_sequence(200)\n",
    "#print(X.shape, Y.shape)\n",
    "#Y_uniq = len(set(Y.numpy()))\n",
    "#print(Y_uniq)\n",
    "\n",
    "#разбиваем матрицу на тестовую, тренировачную и валидационную \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    Y, \n",
    "    test_size=0.30, \n",
    "    random_state=42)\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_test, \n",
    "    y_test, \n",
    "    test_size=0.5, \n",
    "    random_state=42)\n",
    "\n",
    "#print(X_train.shape, y_train.shape)\n",
    "#print(X_test.shape, y_test.shape)\n",
    "\n",
    "BATCH_SIZE = 16  #32 наблюдений (строки)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "data_train = torch.utils.data.DataLoader(dataset, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "data_test = torch.utils.data.DataLoader(dataset, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X_valid, y_valid)\n",
    "data_valid = torch.utils.data.DataLoader(dataset, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "model_RNN = RnnFlex(torch.nn.RNN, 10, 64, 128, 10, 'RNN')\n",
    "model_LSTM = RnnFlex(torch.nn.LSTM, 10, 64, 128, 10, 'LSTM')\n",
    "model_GRU = RnnFlex(torch.nn.GRU, 10, 64, 128, 10, 'GRU')\n",
    "\n",
    "for model in (model_RNN, model_LSTM, model_GRU):\n",
    "    print()\n",
    "    print('model is {}'.format(model.name))\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())   \n",
    "    for ep in range(100):\n",
    "\n",
    "        train_loss = 0.\n",
    "        train_passed = 0\n",
    "        train_acc_sum = 0\n",
    "        n_train = 0\n",
    "\n",
    "        model.train()\n",
    "        for X_b, y_b in data_train:\n",
    "            #print(type(X_b), type(y_b))\n",
    "            optimizer.zero_grad()\n",
    "            #print(X_b.shape, y_b.shape) \n",
    "            #print(X_b, y_b)   \n",
    "            answers = model(X_b)\n",
    "            if model.name == 'LSTM':          \n",
    "                answers = answers.squeeze()           \n",
    "            #answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
    "            #print(y_b.shape)\n",
    "       \n",
    "            predicted = sample(answers)\n",
    "            #print(X_b.shape, y_b.shape, answers.shape, predicted.shape)\n",
    "            #print(predicted, y_b)\n",
    "            train_acc_sum += (predicted == y_b).sum().item()\n",
    "            n_train += y_b.shape[0]\n",
    "\n",
    "            loss = criterion(answers, y_b)\n",
    "\n",
    "            #print(predicted)\n",
    "            #print(y_b)\n",
    "            #print(train_acc_sum)\n",
    "            #print(y_b.shape[0])\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_passed += 1\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        test_acc_sum = 0\n",
    "        n_test = 0\n",
    "\n",
    "        for X_test, y_test in data_test:\n",
    "            #print(X_test.shape, y_test.shape)\n",
    "            answers = model(X_test) \n",
    "            if model.name == 'LSTM':\n",
    "                answers = answers.squeeze()\n",
    "            predicted = sample(answers)\n",
    "            #print(predicted, y_test)\n",
    "            test_acc_sum += (predicted == y_test).sum().item()\n",
    "            n_test += y_test.shape[0]\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            print(\"Epoch {}    Train Loss: {:.3f}  Train acc: {:.3f}  Test acc: {:.3f}\".format(ep, train_loss / train_passed, train_acc_sum / n_train, test_acc_sum / n_test))\n",
    "\n",
    "    valid_acc_sum = 0\n",
    "    n_valid = 0\n",
    "\n",
    "    for X_valid, y_valid in data_valid:\n",
    "        #print(X_test.shape, y_test.shape)\n",
    "        answers = model(X_valid) \n",
    "        if model.name == 'LSTM':\n",
    "            answers = answers.squeeze()\n",
    "        #print(predicted.shape)\n",
    "        predicted = sample(answers)\n",
    "        valid_acc_sum += (predicted == y_valid).sum().item()\n",
    "        n_valid += y_valid.shape[0]\n",
    "\n",
    "    print(\"Valid acc: {:.3f}\".format(valid_acc_sum / n_valid))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pythonProject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a04b0e96a39001dcde3ef110d9aab7edc50a87692e5c15032a80ed4296751e1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
