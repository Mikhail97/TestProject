{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 451436 sentence pairs\n",
      "Trimmed to 451436 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 16741\n",
      "rus 56729\n",
      "0m 11s (- eta: 1m 45s) (100 10%) 6.3424  train_acc = 0.250\n",
      "0m 24s (- eta: 1m 37s) (200 20%) 5.8945  train_acc = 0.667\n",
      "0m 36s (- eta: 1m 25s) (300 30%) 5.7738  train_acc = 0.400\n",
      "0m 49s (- eta: 1m 14s) (400 40%) 5.8043  train_acc = 0.286\n",
      "1m 2s (- eta: 1m 2s) (500 50%) 5.7113  train_acc = 0.000\n",
      "1m 15s (- eta: 0m 50s) (600 60%) 5.6519  train_acc = 0.167\n",
      "1m 29s (- eta: 0m 38s) (700 70%) 5.5530  train_acc = 0.000\n",
      "1m 42s (- eta: 0m 25s) (800 80%) 5.5163  train_acc = 0.000\n",
      "1m 55s (- eta: 0m 12s) (900 90%) 5.5093  train_acc = 0.000\n",
      "2m 8s (- eta: 0m 0s) (1000 100%) 5.2993  train_acc = 0.200\n",
      "\n",
      "\n",
      "> i just got promoted .\n",
      "= меня только что повысили .\n",
      "< том не не не . <EOS>   Learning inference Acc: 0.333\n",
      "\n",
      "> i think it s time for me to write my mother another letter .\n",
      "= думаю пора мне написать матери еще одно письмо .\n",
      "< том не не не . <EOS>   Learning inference Acc: 0.000\n",
      "\n",
      "> tom doesn t eat oranges .\n",
      "= том не ест апельсины .\n",
      "< том не не не . <EOS>   Learning inference Acc: 0.667\n",
      "\n",
      "> how did you find out it was my birthday ?\n",
      "= как ты узнал что сегодня у меня день рождения ?\n",
      "< ты не как как ? <EOS>   Learning inference Acc: 0.000\n",
      "\n",
      "> his mistake was intentional .\n",
      "= его ошибка была преднамереннои .\n",
      "< том не не не . <EOS>   Learning inference Acc: 0.333\n",
      "\n",
      "> may i have a moment of your time ?\n",
      "= можешь уделить мне минуту своего времени ?\n",
      "< ты не как как ? <EOS>   Learning inference Acc: 0.000\n",
      "\n",
      "> we had no idea that tom was going to do that .\n",
      "= мы понятия не имели что том собирается это сделать .\n",
      "< том не не не . <EOS>   Learning inference Acc: 0.167\n",
      "\n",
      "> we can both do it .\n",
      "= мы оба можем это сделать .\n",
      "< том не не не . <EOS>   Learning inference Acc: 0.000\n",
      "\n",
      "> i tried not to think about that .\n",
      "= я пытался не думать об этом .\n",
      "< том не не не . <EOS>   Learning inference Acc: 0.167\n",
      "\n",
      "> do you know why tom doesn t trust mary ?\n",
      "= знаешь почему том не доверяет мэри ?\n",
      "< ты не как как ? <EOS>   Learning inference Acc: 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from doctest import FAIL_FAST\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class CreateDataset():\n",
    "    def read_csv(self, path):\n",
    "        df = pd.read_csv(path, sep='\\t', header=None)\n",
    "        df = df[[0,1]]\n",
    "        return df\n",
    "\n",
    "# Определим по умолчанию 2 токена которые будут нам информировать о начале предложения и конце предложения (SOS и EOS):\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "# Создадим объект словаря нашего языка, который будет хранить данные по маппингу слов - index2word и обратно word2index и плюс второстепенные методы по добавлению токена и обработке предложений:\n",
    "class LanguageVocabulary(object):\n",
    "    def __init__(self, name):\n",
    "        # название языка\n",
    "        self.name = name\n",
    "        # словарик word2index который хранит соответственно кодировку слова в целочисленный индекс словаря\n",
    "        self.word2index = {}\n",
    "        # обычный словарик который хранит распределение слов, сколько слов мы использовали и сколько обнаружили\n",
    "        self.word2count = {}\n",
    "        # Обратный словарик словарю word2index где хранятся уже индексы и замаппенные слова к каждому индексу, нужен будет для расшифровки последовательности\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        # Count SOS and EOS, храним просто общее количество слов в нашем словаре, то есть количество токенов в сформированном словарике нашего языка\n",
    "        self.n_words = 2\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Метод класса, для добавления предложения в словарь.\n",
    "        Каждое предложение поступающее к нам, будет разбираться на\n",
    "        примитивные токены и добавляться в словарь при помощи метода класса addword()\n",
    "        \"\"\"\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # проверяем не входит ли наше слово в словарь word2index\n",
    "        if word not in self.word2index:\n",
    "            # добавляем в качестве ключа слово а в качестве значения последнее n_words\n",
    "            self.word2index[word] = self.n_words\n",
    "            # меняем на единичку\n",
    "            self.word2count[word] = 1\n",
    "            # и соответственно меняем и index2word словарик добавляя уже слово для декодирования\n",
    "            self.index2word[self.n_words] = word\n",
    "            # инкрементируем n_words\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            # Если такое уже слово есть просто добавляем 1 что добавилось одно слово\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "            # Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    # Декодируем из юникода в ascii\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    # Что означает данное регулярное выражение - точку, !, ? меняем на пробел чтобы этот символ стоял отдельно от всех\n",
    "    # https://docs.python.org/3/library/re.html - стандартная (родная) библиотка Python которая нужна для работы с регулярными выражениями\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # оставляем только наборы символов указанных в паттерне регулярного выражения остальное заменим на пробел\n",
    "    s = re.sub(r\"[^a-zA-Zа-яА-ЯёЁĄąĆćĘęŁłŃńÓóŚśŹźŻżÄäÖöÜü\\u4e00-\\u9fff.!?]+\", r\" \", s)\n",
    "   \n",
    "    return s\n",
    "\n",
    "\n",
    "def read_languages(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    # Берем документ корпуса, лежащий в директории ./data/___.txt подставляя значения указанных языков в нашем случае eng-fra, он читается бьется на предложения\n",
    "    #lines = open('/home/mikhail/it-academy/модуль 5/lecture_7_deepRNN/data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "    lines = [l[0]+'\\t'+l[1] for l in df.values]\n",
    "    #print(lines[:10])\n",
    "    # Разбиваем построчно и нормализуем строку:\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    # Можем создавать и проходить как с целевого языка на исходный так и наоборот:\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = LanguageVocabulary(lang2)\n",
    "        output_lang = LanguageVocabulary(lang1)\n",
    "    else:\n",
    "        input_lang = LanguageVocabulary(lang1)\n",
    "        output_lang = LanguageVocabulary(lang2)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "\"\"\"\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filter_pair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]\n",
    "\"\"\"\n",
    "\n",
    "def prepare_data(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_languages(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    #pairs = filter_pairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # Как помним hidden_size - размер скрытого состояния\n",
    "        self.hidden_size = hidden_size\n",
    "        # Слой Эмбеддингов, который из входного вектора последовательности (либо батча) отдаст представление последовательности для скрытого состояния\n",
    "        # FYI: в качестве Input_size у нас размер словаря\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # И соответственно рекуррентная ячейка GRU которая принимает MxM (hidden на hidden)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Приводим эмбеддинг к формату одного предлоежния 1х1 и любая размерность\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # Нужно для следующего шага пока не запутываемся :) просто присвоили наш эмбеддинг\n",
    "        output = embedded\n",
    "        # и соответственно подаем все в ГРЮ ячейку (эмбеддинг и скрытые состояния)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        # Дополнительно сделаем инициализацию скрытого представления (просто заполним нулями)\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    # Будьте внимательны, теперь на вход мы получаем размер скрытого представления\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Слой эмбеддингов - рамер словаря, размер скрытого представления\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # GRU скрытое состояние на скрытое\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        # Переводим hidden size в распределение для этого передаем в линейный слов скрытое состояни и размер словаря\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        # Получаем распределение верояностей\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0])) # берем output по нулевому индексу (одно предложение)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "# Токены кодируем в целочисленное представление\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    #print(sentence)\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "# Берем предложение с указанным языком, делаем из него индексы и вставляем метку конца предложения, превращаем в тензор:\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    #print('indexes = ',indexes)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "# Для создания тензора из пар:\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    #print('input_tensor = ', input_tensor)\n",
    "    #print('tensorsFromPair shape')\n",
    "    #print(input_tensor.shape)\n",
    "    #print(target_tensor.shape)\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    # Просто инициализируем скрытое представление для энкодера\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    # Скиыдваем градиенты для алгоритма градиентного спуска как и у энкодера так и у дэкодера\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Получаем размер в словаря (токенов) для входящего и выходящего тензора так как мы пробегаемся по каждому предложению по кусочкам\n",
    "    #print('input_tensor.shape = ', input_tensor.shape)\n",
    "    input_length = input_tensor.size(0)\n",
    "    #print('target_length = ',target_tensor.size(0))\n",
    "    target_length = target_tensor.size(0)\n",
    "    max_length = input_length\n",
    "    #print('input_length = {}  target_length = {} '.format(input_length,target_length))\n",
    "    # Создаем переменную где будем хранить наши выходы из энкодера (в данной реализации пока не юзаем, далее будет еще один вариант)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    loss = 0\n",
    "    # пробегаем по длине входящего тензора и в экодер передаем последовательно каждый из токенов:\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        # Сохраняем все выходы из энкодера для одного слова\n",
    "        #print('encoder_output = ',encoder_output,encoder_output[0, 0])\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "\n",
    "    # Закончили с энкодером пошли к декодеру, как было сказано декодер начинается с SOS\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    # FYI здесь мы скрытое представление из энкодера передаем в скрытое представление в декодер, то есть после знака =\n",
    "    # у нас будут ходить градиенты из декодера в энкодер, то есть когда мы будем считать градиенты, они сначала пробегут по декодеру\n",
    "    # дойдут до знака = перескочат в энкодер и будут дальше считаться по энкодеру и эти градиенты сохранятся в соответствующих тензорах\n",
    "    # и когда будут отрабатывать разные оптимайзеры (у нас их 2) у них будут соответствующие правильные градиенты которые смогут правильно отработать\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Будем использовать Teacher Forcing в части случае (подставляя правильную последовательность)\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        train_acc_sum = 0\n",
    "        n_train = 0\n",
    "\n",
    "        #train_acc_sum_top = 0\n",
    "        #n_train_top = 0\n",
    "\n",
    "        # Подаем decoder_input = torch.tensor([[SOS_token]], device=device) то есть по одному слову и скрытое представление\n",
    "        for di in range(target_length):\n",
    "            # Переведенное предложение и скрытое представление\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            #print('decoder', decoder_output)\n",
    "            #softmaxed = torch.softmax(decoder_output, 1) \n",
    "            #probas = torch.distributions.multinomial.Multinomial(1, softmaxed).sample()  # нормализация\n",
    "            #probas.max(dim=1)[1]\n",
    "            #print('probas.argmax = ',probas.argmax(dim=1), target_tensor[di])\n",
    "            #train_acc_sum += (probas.max(dim=1)[1] == target_tensor[di].item()).sum().item()\n",
    "            #n_train += target_tensor[di].shape[0]\n",
    "            # Считаем ошибку\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "            \n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            #print('topi = ',topi,target_tensor[di].item())\n",
    "            train_acc_sum += (topi == target_tensor[di].item()).sum().item()\n",
    "            n_train += target_tensor[di].shape[0]  \n",
    "        #print('train_acc_sum_top = {}  n_train_top = {}    Train_acc_top: {:.3f}'.format(train_acc_sum_top, n_train_top, train_acc_sum_top / n_train_top))\n",
    "    else:\n",
    "        train_acc_sum = 0\n",
    "        n_train = 0\n",
    "        #train_acc_sum_top = 0\n",
    "        #n_train_top = 0\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            \n",
    "            #сначало смотрел acc таким образом\n",
    "            #softmaxed = torch.softmax(decoder_output, 1) \n",
    "            #probas = torch.distributions.multinomial.Multinomial(1, softmaxed).sample()  # нормализация\n",
    "            #train_acc_sum += (probas.max(dim=1)[1] == target_tensor[di].item()).sum().item()\n",
    "            #n_train += target_tensor[di].shape[0]\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "\n",
    "            #смотрим acc\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            #print('topi = ',topi,target_tensor[di].item())\n",
    "            train_acc_sum += (topi == target_tensor[di].item()).sum().item()\n",
    "            n_train += target_tensor[di].shape[0]  \n",
    "\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "        #print('train_acc_sum_top = {}  n_train_top = {}    Train_acc_top: {:.3f}'.format(train_acc_sum_top, n_train_top, train_acc_sum_top / n_train_top))\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item() / target_length,  train_acc_sum / n_train\n",
    "\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / percent\n",
    "    rs = es - s\n",
    "    return '%s (- eta: %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    # Делаем выборку наших пар функцией которую создали до\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]#[ tensorsFromPair(['tom was wearing an orange jumpsuit and his hands were cuffed in front of him .',\n",
    "                      #                  'том был одет в оранжевыи комбинезон и его руки были скованы спереди .'])] \n",
    "                                        # [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "    #print('training_pairs = ',training_pairs)\n",
    "    #print('training_pairs ', training_pairs[0])\n",
    "    # FYI! Используем Negative Log-Likelihood Loss потому что log softmax уже присутствует в модели\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[epoch - 1]\n",
    "        #print('training_pair = ',training_pair)\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        # Используем функцию для тренировки на отдельных токенах, которую написали выше\n",
    "        #print('input_tensor',input_tensor.shape)\n",
    "        loss, train_acc = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f  train_acc = %.3f' % (timeSince(start, epoch / n_iters),\n",
    "                                         epoch, epoch / n_iters * 100, print_loss_avg, train_acc))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    showPlot(plot_losses)\n",
    "\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, out_sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        output_tensor = tensorFromSentence(output_lang, out_sentence)\n",
    "        #print('input_tensor = ',input_tensor.shape, output_tensor.shape)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        max_length = input_length\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "            encoder_outputs[i] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = [] # Наши деокдированные слова\n",
    "\n",
    "        acc_sum = 0\n",
    "        n = 0\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            #print('topi.item() = ',topi.item())\n",
    "            #softmaxed = torch.softmax(decoder_output, 1) \n",
    "            #probas = torch.distributions.multinomial.Multinomial(1, softmaxed).sample()  # нормализация\n",
    "            #probas.max(dim=1)[1]\n",
    "            #print('probas.argmax = ',topi, output_tensor[di].item())\n",
    "            acc_sum += (topi == output_tensor[di].item()).sum().item()\n",
    "            #print(output_tensor[di].shape[0])\n",
    "            n += output_tensor[di].shape[0]            \n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        #print(decoded_words)\n",
    "        #print('Learning inference Acc: {:.3f}'.format(train_acc_sum / n_train))\n",
    "        return decoded_words, acc_sum / n\n",
    "\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, acc = evaluate(encoder, decoder, pair[0], pair[1])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('< {}   Learning inference Acc: {:.3f}'.format(output_sentence, acc))\n",
    "        print('')\n",
    "\n",
    "dataset = CreateDataset()\n",
    "path = '/home/mikhail/it-academy/модуль 5/lecture_7_deepRNN/data/translation_data/rus.txt'\n",
    "df = dataset.read_csv(path)   \n",
    "#df = df[:1]\n",
    "#print(df)\n",
    "input_lang, output_lang, pairs = prepare_data('eng', 'rus', False)\n",
    "#print('pairs = ',pairs, input_lang.n_words, output_lang.n_words)\n",
    "\n",
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "#print(encoder1)\n",
    "#print(decoder1)\n",
    "trainIters(encoder1, decoder1, 450000, print_every=45000) #75000 число итераций\n",
    "\n",
    "print()\n",
    "print()\n",
    "evaluateRandomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> the new park is due to be open in october .\n",
      "= новыи парк должен быть открыт в октябре .\n",
      "< в парк в в парк . <EOS>   Learning inference Acc: 0.143\n",
      "\n",
      "> i fell in love with someone my parents didn t like .\n",
      "= я влюбилась в человека которыи не нравится моим родителям .\n",
      "< я не не в в не не . <EOS>   Learning inference Acc: 0.222\n",
      "\n",
      "> things didn t turn out the way tom expected .\n",
      "= все вышло не так как том ожидал .\n",
      "< не не не том не как <EOS>   Learning inference Acc: 0.143\n",
      "\n",
      "> i m afraid it s going to rain tomorrow .\n",
      "= боюсь что завтра будет дождь .\n",
      "< боюсь завтра дождь дождь . <EOS>   Learning inference Acc: 0.167\n",
      "\n",
      "> do you really want to marry me ?\n",
      "= вы деиствительно хотите на мне жениться ?\n",
      "< вы деиствительно хотите на меня ? <EOS>   Learning inference Acc: 0.571\n",
      "\n",
      "> it was a competition .\n",
      "= это было соревнование .\n",
      "< это было ошибкои . <EOS>   Learning inference Acc: 0.800\n",
      "\n",
      "> i haven t read many books .\n",
      "= я не так много книг читал .\n",
      "< я не читал книг книг . <EOS>   Learning inference Acc: 0.429\n",
      "\n",
      "> a luxury liner arrived in the harbor .\n",
      "= в порт прибыл роскошныи лаинер .\n",
      "< на корабль на в <EOS>   Learning inference Acc: 0.000\n",
      "\n",
      "> tom cleaned out his bedroom closet .\n",
      "= том убрал в шкафу своеи спальни .\n",
      "< том в свою шкафу в . <EOS>   Learning inference Acc: 0.286\n",
      "\n",
      "> there s a large clock near the top of the tower .\n",
      "= недалеко от вершины башни есть большие часы .\n",
      "< большая большая недалеко от новых . <EOS>   Learning inference Acc: 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(encoder, decoder, sentence, out_sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        output_tensor = tensorFromSentence(output_lang, out_sentence)\n",
    "        #print('input_tensor = ',input_tensor)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        max_length = input_length\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "            encoder_outputs[i] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = [] # Наши деокдированные слова\n",
    "\n",
    "        acc_sum = 0\n",
    "        n = 0\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            #print('topi.item() = ',topi.item())\n",
    "            #softmaxed = torch.softmax(decoder_output, 1) \n",
    "            #probas = torch.distributions.multinomial.Multinomial(1, softmaxed).sample()  # нормализация\n",
    "            #probas.max(dim=1)[1]\n",
    "            #print('probas.argmax = ',topi, output_tensor[di].item())\n",
    "            acc_sum += (topi == output_tensor[di].item()).sum().item()\n",
    "            n += input_tensor[di].shape[0]            \n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        #print(decoded_words)\n",
    "        #print('Learning inference Acc: {:.3f}'.format(train_acc_sum / n_train))\n",
    "        return decoded_words, acc_sum / n\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, acc = evaluate(encoder, decoder, pair[0], pair[1])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('< {}   Learning inference Acc: {:.3f}'.format(output_sentence, acc))\n",
    "        print('')\n",
    "\n",
    "evaluateRandomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ахнул'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.index2word[1998]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pythonProject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a04b0e96a39001dcde3ef110d9aab7edc50a87692e5c15032a80ed4296751e1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
